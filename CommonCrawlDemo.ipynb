{
 "metadata": {
  "name": "",
  "signature": "sha256:33bef0396ac7ba705f5d0d78edfd52182626c7243851dde4094aa94071fbe81c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Common Crawl demo"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "To run this demo you must install and configure boto library. Ensure that boto has access to your AWS credentials."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cclib import commoncrawl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Listing Crawls & Crawl Identifiers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Common Crawl project crawls the web every few months. Following is a list of Crawls which can be currently retrived using the library. Note that craws before 2013 used Hadoop Sequence file format instead of the current WARC format and cannot be retrived using this library"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Crawl ID'\n",
      "print '\\n'.join(commoncrawl.CommonCrawl.crawl_id_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Crawl ID\n",
        "2013_1\n",
        "2013_2\n",
        "2014_1\n",
        "ALL\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The 'ALL' identifiers contains files from all crawls."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CRAWLS = { crawl_id : commoncrawl.CommonCrawl(crawl_id) for crawl_id in commoncrawl.CommonCrawl.crawl_id_list }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Getting list of files in each crawl"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '\\t'.join(['ID','WARC','WET','WAT','text(only first crawl in 2013)'])\n",
      "for crawlid, crawl in CRAWLS.iteritems():\n",
      "    print '\\t'.join([crawlid,str(len(crawl.warc)),str(len(crawl.wet)),str(len(crawl.wat)),str(len(crawl.text))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ID\tWARC\tWET\tWAT\ttext(only first crawl in 2013)\n",
        "ALL\t139200\t132462\t132462\t31600\n",
        "2014_1\t55700\t55699\t55699\t0\n",
        "2013_1\t31600\t31568\t31568\t31600\n",
        "2013_2\t51900\t45195\t45195\t0\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Accessing Common Crawl files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reading a WAT (Metadata) file from the second crawl in 2013 and printing first 25 lines from it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "crawl_id = '2013_2'\n",
      "example_wat_filename = random.choice(CRAWLS[crawl_id].wat)\n",
      "count = 50\n",
      "example_wat_fileobject = CRAWLS[crawl_id].get_file(example_wat_filename,headers = {'Range': 'bytes=0-100000'})\n",
      "for line in example_wat_fileobject:\n",
      "    if line.strip():\n",
      "        count -=1\n",
      "        if count < 10:\n",
      "            print line[:100]\n",
      "        if count == 0:\n",
      "            break\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WARC-Record-ID: <urn:uuid:721a0c4d-82f9-450e-a7c8-3a7232894883>\r\n",
        "\n",
        "WARC-Refers-To: <urn:uuid:6393c216-0fb9-405b-a749-d47029bf3e23>\r\n",
        "\n",
        "Content-Type: application/json\r\n",
        "\n",
        "Content-Length: 1120\r\n",
        "\n",
        "{\"Envelope\":{\"Format\":\"WARC\",\"WARC-Header-Length\":\"445\",\"Block-Digest\":\"sha1:VW6RU4CDMZHRI742K45S4SR\n",
        "WARC/1.0\r\n",
        "\n",
        "WARC-Type: metadata\r\n",
        "\n",
        "WARC-Target-URI: http://0pointer.de/photos/?gallery=Amazon%202010-03&photo=945&exif_style=&show_thum\n",
        "WARC-Date: 2013-12-12T23:09:51Z\r\n",
        "\n",
        "WARC-Record-ID: <urn:uuid:37cdc3cf-986e-4bb0-9bbb-634c72a03b80>\r\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reading a WET (Text) file from the second crawl in 2013 and printing first 25 lines from it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crawl_id = '2013_2'\n",
      "example_wet_filename = random.choice(CRAWLS[crawl_id].wet)\n",
      "count = 50\n",
      "example_wet_fileobject = CRAWLS[crawl_id].get_file(example_wet_filename,headers = {'Range': 'bytes=0-100000'})\n",
      "for line in example_wet_fileobject:\n",
      "    if line.strip():\n",
      "        count -=1\n",
      "        if count < 20:\n",
      "            print line[:100]\n",
      "        if count == 0:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "links\n",
        "\n",
        "reviews\n",
        "\n",
        "subscribe\n",
        "\n",
        "Birds\n",
        "\n",
        "Birding\n",
        "\n",
        "Trips\n",
        "\n",
        "Blogging\n",
        "\n",
        "Conservation\n",
        "\n",
        "Destinations\n",
        "\n",
        "Inspiration\n",
        "\n",
        "Reviews\n",
        "\n",
        "Politics\n",
        "\n",
        "Sports\n",
        "\n",
        "News\n",
        "\n",
        "Get Ready for the GBBC and Win a Free Book\n",
        "\n",
        "February 06, 2008\n",
        "\n",
        "Mike\n",
        "\n",
        "Birding\n",
        "\n",
        "36 Comments\n",
        "\n",
        "We\u2019re entering the home stretch on winter here in North America, which means it must be time for t\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Example worker function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following is an example of a simple worker function which takes a WAT file and extracts list URLs. Note that this is not part of the library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from worker import process_file\n",
      "from pprint import pprint\n",
      "data = process_file(example_wat_fileobject,example_wat_filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR:root:error while processing file\n",
        "Traceback (most recent call last):\n",
        "  File \"worker.py\", line 49, in process_file\n",
        "    for line in fileobj:\n",
        "  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 450, in readline\n",
        "    c = self.read(readsize)\n",
        "  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 256, in read\n",
        "    self._read(readsize)\n",
        "  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 303, in _read\n",
        "    self._read_eof()\n",
        "  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 342, in _read_eof\n",
        "    hex(self.crc)))\n",
        "IOError: CRC check failed 0xae79b42 != 0xfd137780L\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ignore the error above which is produced due to the fact that only first few thousand bytes were downloaded. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'amazon': [],\n",
        " 'counts': [],\n",
        " 'error': True,\n",
        " 'filename': 'common-crawl/crawl-data/CC-MAIN-2013-48/segments/1386164746201/wat/CC-MAIN-20131204134546-00047-ip-10-33-133-15.ec2.internal.warc.wat.gz',\n",
        " 'metdata_lines': 40}\n"
       ]
      }
     ],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}